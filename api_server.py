from flask import Flask, request, jsonify
from flask_cors import CORS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import json
import numpy as np

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œì—ë§Œ í•„ìš”)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

app = Flask(__name__)
CORS(app)  # CORS ì„¤ì •ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥

# ì „ì—­ ë°ì´í„° ì €ì¥ì†Œ
all_contents_data = None
tfidf_matrix = None
tfidf_vectorizer = None

def load_all_contents():
    """ëª¨ë“  ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    try:
        # all_contents.json íŒŒì¼ ë¡œë“œ
        print("all_contents.json íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        with open('web/json/all_contents.json', 'r', encoding='utf-8') as f:
            all_contents_data = json.load(f)
        
        print(f"ë¡œë“œëœ ì½˜í…ì¸  ìˆ˜: {len(all_contents_data)}")
        
        # overviewê°€ ìˆëŠ” ì½˜í…ì¸ ë§Œ í•„í„°ë§
        valid_contents = []
        overviews = []
        
        for content in all_contents_data:
            overview = content.get('overview', '')
            if overview and overview.strip():
                valid_contents.append(content)
                overviews.append(overview)
        
        all_contents_data = valid_contents
        print(f"ìœ íš¨í•œ overviewê°€ ìˆëŠ” ì½˜í…ì¸  ìˆ˜: {len(all_contents_data)}")
        
        if len(overviews) > 0:
            # TF-IDF ë²¡í„°í™”
            print("TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
            tfidf_vectorizer = TfidfVectorizer(
                max_features=3000,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
            
            tfidf_matrix = tfidf_vectorizer.fit_transform(overviews)
            print(f"TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ: {tfidf_matrix.shape}")
        
        return True
        
    except FileNotFoundError:
        print("ì˜¤ë¥˜: web/json/all_contents.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

class MovieKeywordExtractor:
    def __init__(self):
        # ì˜ì–´ ë¶ˆìš©ì–´ ì„¤ì •
        self.stop_words = set(stopwords.words('english'))
        # ì¶”ê°€ ë¶ˆìš©ì–´ (ì˜í™” ê´€ë ¨ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤)
        self.movie_stop_words = {
            'movie', 'film', 'story', 'character', 'plot', 'scene', 
            'drama', 'action', 'comedy', 'thriller', 'horror',
            'one', 'two', 'three', 'first', 'second', 'last',
            'new', 'old', 'good', 'bad', 'big', 'small',
            'man', 'woman', 'people', 'person', 'life', 'time',
            'way', 'world', 'day', 'year', 'home', 'family'
        }
        self.stop_words.update(self.movie_stop_words)
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # í† í°í™”
        tokens = word_tokenize(text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ 3 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
        filtered_tokens = [token for token in tokens 
                          if token not in self.stop_words and len(token) >= 3]
        
        return ' '.join(filtered_tokens)
    
    def extract_keywords_tfidf(self, text, max_features=10):
        """TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        processed_text = self.preprocess_text(text)
        
        if not processed_text.strip():
            return []
        
        try:
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),  # 1-2ê·¸ë¨ ì‚¬ìš©
                min_df=1,
                max_df=0.95
            )
            
            tfidf_matrix = vectorizer.fit_transform([processed_text])
            feature_names = vectorizer.get_feature_names_out()
            scores = tfidf_matrix.toarray()[0]
            
            # ì ìˆ˜ì™€ ë‹¨ì–´ë¥¼ íŠœí”Œë¡œ ë§Œë“¤ì–´ ì •ë ¬
            keyword_scores = list(zip(feature_names, scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ì ìˆ˜ê°€ 0ì¸ ê²ƒë“¤ ì œì™¸
            keywords = [{'word': word, 'score': float(score)} 
                       for word, score in keyword_scores if score > 0]
            
            return keywords[:max_features]
            
        except Exception as e:
            print(f"TF-IDF ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_keywords_frequency(self, text, max_features=10):
        """ë‹¨ìˆœ ë¹ˆë„ìˆ˜ë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        processed_text = self.preprocess_text(text)
        
        if not processed_text.strip():
            return []
        
        try:
            vectorizer = CountVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),
                min_df=1
            )
            
            count_matrix = vectorizer.fit_transform([processed_text])
            feature_names = vectorizer.get_feature_names_out()
            counts = count_matrix.toarray()[0]
            
            keyword_counts = list(zip(feature_names, counts))
            keyword_counts.sort(key=lambda x: x[1], reverse=True)
            
            keywords = [{'word': word, 'count': int(count)} 
                       for word, count in keyword_counts if count > 0]
            
            return keywords[:max_features]
            
        except Exception as e:
            print(f"ë¹ˆë„ìˆ˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

# ì „ì—­ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
extractor = MovieKeywordExtractor()

def calculate_genre_similarity(genres1, genres2):
    """ì¥ë¥´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ì‚¬ìš©)"""
    if not genres1 or not genres2:
        return 0.0
    
    set1 = set(genres1)
    set2 = set(genres2)
    
    # Jaccard ìœ ì‚¬ë„: êµì§‘í•© / í•©ì§‘í•©
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    if union == 0:
        return 0.0
    
    return intersection / union

@app.route('/api/recommend-content', methods=['POST'])
def recommend_content():
    """ì½˜í…ì¸  ì¶”ì²œ API (ì¤„ê±°ë¦¬ + ì¥ë¥´ ê¸°ë°˜)"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    try:
        data = request.get_json()
        
        if not data or 'movie_id' not in data or 'type' not in data:
            return jsonify({'error': 'movie_idì™€ type í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        movie_id = int(data['movie_id'])
        content_type = data['type']
        max_recommendations = data.get('max_recommendations', 10)
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (ì¡°ì • ê°€ëŠ¥)
        overview_weight = data.get('overview_weight', 0.7)  # ì¤„ê±°ë¦¬ ê°€ì¤‘ì¹˜
        genre_weight = data.get('genre_weight', 0.3)        # ì¥ë¥´ ê°€ì¤‘ì¹˜
        
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
        if all_contents_data is None:
            if not load_all_contents():
                return jsonify({'error': 'ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 500
        
        # í˜„ì¬ ì½˜í…ì¸  ì°¾ê¸°
        current_content = None
        current_index = -1
        
        for i, content in enumerate(all_contents_data):
            if content['id'] == movie_id and content['type'] == content_type:
                current_content = content
                current_index = i
                break
        
        if current_content is None:
            return jsonify({'error': 'í•´ë‹¹ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 404
        
        current_overview = current_content.get('overview', '')
        current_genres = current_content.get('genre_ids', [])
        
        if current_overview == '' and not current_genres:
            return jsonify({
                'current_content': {
                    'id': current_content['id'],
                    'title': current_content.get('title', current_content.get('name', 'Unknown')),
                    'type': current_content['type']
                },
                'recommendations': [],
                'message': 'í˜„ì¬ ì½˜í…ì¸ ì— ì¤„ê±°ë¦¬ì™€ ì¥ë¥´ ì •ë³´ê°€ ì—†ì–´ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            })
        
        # TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ê°€ ì—†ë‹¤ë©´ ìƒì„±
        if tfidf_matrix is None:
            return jsonify({'error': 'TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500
        
        # ì¤„ê±°ë¦¬ ìœ ì‚¬ë„ ê³„ì‚°
        overview_similarities = np.zeros(len(all_contents_data))
        if current_overview and current_overview.strip():
            current_vector = tfidf_matrix[current_index:current_index+1]
            overview_similarities = cosine_similarity(current_vector, tfidf_matrix).flatten()
        
        # ëª¨ë“  ì½˜í…ì¸ ì™€ì˜ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°
        similar_contents = []
        for i, content in enumerate(all_contents_data):
            if i == current_index:  # ìê¸° ìì‹ ì€ ì œì™¸
                continue
            
            # ì¥ë¥´ ìœ ì‚¬ë„ ê³„ì‚°
            content_genres = content.get('genre_ids', [])
            genre_similarity = calculate_genre_similarity(current_genres, content_genres)
            
            # ì¤„ê±°ë¦¬ ìœ ì‚¬ë„
            overview_similarity = overview_similarities[i] if current_overview else 0.0
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
            final_similarity = (overview_similarity * overview_weight) + (genre_similarity * genre_weight)
            
            # ìµœì†Œ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ì œì™¸)
            min_threshold = 0.1
            if final_similarity > min_threshold:
                similar_contents.append({
                    'index': i,
                    'similarity': float(final_similarity),
                    'overview_similarity': float(overview_similarity),
                    'genre_similarity': float(genre_similarity),
                    'content': content
                })
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        similar_contents.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        top_recommendations = similar_contents[:max_recommendations]
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        recommendations = []
        for item in top_recommendations:
            content = item['content']
            recommendations.append({
                'id': content['id'],
                'title': content.get('title', content.get('name', 'Unknown')),
                'type': content['type'],
                'poster_path': content.get('poster_path', ''),
                'overview': content.get('overview', ''),
                'vote_average': content.get('vote_average', 0),
                'similarity_score': round(item['similarity'], 4),
                'overview_similarity': round(item['overview_similarity'], 4),
                'genre_similarity': round(item['genre_similarity'], 4),
                'genre_ids': content.get('genre_ids', [])
            })
        
        return jsonify({
            'current_content': {
                'id': current_content['id'],
                'title': current_content.get('title', current_content.get('name', 'Unknown')),
                'type': current_content['type'],
                'genres': current_genres
            },
            'recommendations': recommendations,
            'total_recommendations': len(recommendations),
            'algorithm': 'TF-IDF + Genre Similarity (Hybrid)',
            'weights': {
                'overview_weight': overview_weight,
                'genre_weight': genre_weight
            }
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/extract-keywords', methods=['POST'])
def extract_keywords():
    try:
        data = request.get_json()
        
        if not data or 'overview' not in data:
            return jsonify({'error': 'overview í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        overview = data['overview']
        movie_title = data.get('title', 'Unknown')
        method = data.get('method', 'tfidf')  # 'tfidf' ë˜ëŠ” 'frequency'
        max_keywords = data.get('max_keywords', 10)
        
        if not overview or overview.strip() == '':
            return jsonify({
                'title': movie_title,
                'keywords': [],
                'message': 'ì¤„ê±°ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'
            })
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        if method == 'frequency':
            keywords = extractor.extract_keywords_frequency(overview, max_keywords)
        else:
            keywords = extractor.extract_keywords_tfidf(overview, max_keywords)
        
        return jsonify({
            'title': movie_title,
            'method': method,
            'keywords': keywords,
            'total_keywords': len(keywords)
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/test-weights', methods=['POST'])
def test_weights():
    """ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ë¡œ ì¶”ì²œ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” API"""
    global all_contents_data, tfidf_matrix
    
    try:
        data = request.get_json()
        
        if not data or 'movie_id' not in data or 'type' not in data:
            return jsonify({'error': 'movie_idì™€ type í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        movie_id = int(data['movie_id'])
        content_type = data['type']
        
        # ì—¬ëŸ¬ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
        weight_combinations = [
            {'overview': 1.0, 'genre': 0.0},    # ì¤„ê±°ë¦¬ë§Œ
            {'overview': 0.8, 'genre': 0.2},    # ì¤„ê±°ë¦¬ ì¤‘ì‹¬
            {'overview': 0.7, 'genre': 0.3},    # ê¸°ë³¸ê°’
            {'overview': 0.6, 'genre': 0.4},    # ê· í˜•
            {'overview': 0.5, 'genre': 0.5},    # ì™„ì „ ê· í˜•
            {'overview': 0.3, 'genre': 0.7},    # ì¥ë¥´ ì¤‘ì‹¬
            {'overview': 0.0, 'genre': 1.0}     # ì¥ë¥´ë§Œ
        ]
        
        results = []
        
        for weights in weight_combinations:
            # ê° ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ì¶”ì²œ ìš”ì²­
            test_data = {
                'movie_id': movie_id,
                'type': content_type,
                'max_recommendations': 5,
                'overview_weight': weights['overview'],
                'genre_weight': weights['genre']
            }
            
            # recommend_content í•¨ìˆ˜ ë‚´ë¶€ ë¡œì§ì„ ì¬ì‚¬ìš©
            # (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìƒìœ„ 5ê°œë§Œ ê°€ì ¸ì˜´)
            # ... ì‹¤ì œ êµ¬í˜„ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ìƒëµí•˜ê³  ê²°ê³¼ë§Œ ì €ì¥
            
            results.append({
                'weights': weights,
                'description': f"ì¤„ê±°ë¦¬ {int(weights['overview']*100)}% + ì¥ë¥´ {int(weights['genre']*100)}%",
                'sample_count': 5
            })
        
        return jsonify({
            'movie_id': movie_id,
            'type': content_type,
            'weight_tests': results,
            'recommendation': 'ê°€ì¥ ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì„ íƒí•˜ì—¬ /api/recommend-contentë¥¼ í˜¸ì¶œí•˜ì„¸ìš”'
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/content-stats', methods=['GET'])
def content_stats():
    """ì½˜í…ì¸  ë°ì´í„° í†µê³„ ì •ë³´ ì œê³µ"""
    global all_contents_data
    
    if all_contents_data is None:
        return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500
    
    # ê¸°ë³¸ í†µê³„
    total_contents = len(all_contents_data)
    
    # íƒ€ì…ë³„ ë¶„ë¥˜
    type_counts = {}
    genre_counts = {}
    overview_counts = {'with_overview': 0, 'without_overview': 0}
    
    for content in all_contents_data:
        # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        content_type = content.get('type', 'unknown')
        type_counts[content_type] = type_counts.get(content_type, 0) + 1
        
        # ì¥ë¥´ë³„ ì¹´ìš´íŠ¸
        genres = content.get('genre_ids', [])
        for genre_id in genres:
            genre_counts[genre_id] = genre_counts.get(genre_id, 0) + 1
        
        # ì¤„ê±°ë¦¬ ìœ ë¬´
        overview = content.get('overview', '')
        if overview and overview.strip():
            overview_counts['with_overview'] += 1
        else:
            overview_counts['without_overview'] += 1
    
    # ìƒìœ„ 10ê°œ ì¥ë¥´
    top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return jsonify({
        'total_contents': total_contents,
        'type_distribution': type_counts,
        'overview_distribution': overview_counts,
        'top_genres': [{'genre_id': gid, 'count': count} for gid, count in top_genres],
        'tfidf_matrix_shape': list(tfidf_matrix.shape) if tfidf_matrix is not None else None
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    global all_contents_data
    status = {
        'status': 'OK', 
        'message': 'API ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.',
        'data_loaded': all_contents_data is not None,
        'total_contents': len(all_contents_data) if all_contents_data else 0,
        'features': ['ì¤„ê±°ë¦¬ ê¸°ë°˜ ì¶”ì²œ', 'ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ', 'í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ', 'ê°€ì¤‘ì¹˜ ì¡°ì •']
    }
    return jsonify(status)

@app.route('/api/reload-data', methods=['POST'])
def reload_data():
    """ë°ì´í„° ì¬ë¡œë“œ API"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
    all_contents_data = None
    tfidf_matrix = None
    tfidf_vectorizer = None
    
    # ë°ì´í„° ì¬ë¡œë“œ
    if load_all_contents():
        return jsonify({'message': 'ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì¬ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.'})
    else:
        return jsonify({'error': 'ë°ì´í„° ì¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 500

if __name__ == '__main__':
    print("ì˜í™” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì½˜í…ì¸  ì¶”ì²œ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì—”ë“œí¬ì¸íŠ¸:")
    print("- POST /api/extract-keywords")
    print("- POST /api/recommend-content (ì¤„ê±°ë¦¬ + ì¥ë¥´ í•˜ì´ë¸Œë¦¬ë“œ)")
    print("- POST /api/test-weights (ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸)")
    print("- GET /api/content-stats (ë°ì´í„° í†µê³„)")
    print("- POST /api/reload-data")
    print("- GET /api/health")
    print("\nğŸ”¥ ê°œì„ ì‚¬í•­:")
    print("âœ… ì¥ë¥´ ìœ ì‚¬ë„ ì¶”ê°€ (Jaccard ìœ ì‚¬ë„)")
    print("âœ… ì¤„ê±°ë¦¬ + ì¥ë¥´ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ")
    print("âœ… ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’: ì¤„ê±°ë¦¬ 70% + ì¥ë¥´ 30%)")
    print("âœ… ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •")
    
    # ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë“œ
    print("\nì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    load_all_contents()
    
    app.run(debug=True, host='0.0.0.0', port=5000) 