from flask import Flask, request, jsonify
from flask_cors import CORS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import json
import numpy as np
import cv2
import base64
from PIL import Image
from io import BytesIO
from fer import FER
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œì—ë§Œ í•„ìš”)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

app = Flask(__name__)
CORS(app)  # CORS ì„¤ì •ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥

# ì „ì—­ ë°ì´í„° ì €ì¥ì†Œ
all_contents_data = None
tfidf_matrix = None
tfidf_vectorizer = None

# ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” (ì „ì—­ ë³€ìˆ˜)
emotion_detector = None

def initialize_emotion_detector():
    """ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”"""
    global emotion_detector
    try:
        logger.info("ê°ì • ì¸ì‹ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
        emotion_detector = FER(mtcnn=True)  # MTCNN ì‚¬ìš©ìœ¼ë¡œ ë” ì •í™•í•œ ì–¼êµ´ ê°ì§€
        logger.info("âœ… ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
    except Exception as e:
        logger.error(f"âŒ ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def load_all_contents():
    """ëª¨ë“  ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    try:
        # all_contents.json íŒŒì¼ ë¡œë“œ
        print("all_contents.json íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        with open('web/json/all_contents.json', 'r', encoding='utf-8') as f:
            all_contents_data = json.load(f)
        
        print(f"ë¡œë“œëœ ì½˜í…ì¸  ìˆ˜: {len(all_contents_data)}")
        
        # overviewê°€ ìˆëŠ” ì½˜í…ì¸ ë§Œ í•„í„°ë§
        valid_contents = []
        overviews = []
        
        for content in all_contents_data:
            overview = content.get('overview', '')
            if overview and overview.strip():
                valid_contents.append(content)
                overviews.append(overview)
        
        all_contents_data = valid_contents
        print(f"ìœ íš¨í•œ overviewê°€ ìˆëŠ” ì½˜í…ì¸  ìˆ˜: {len(all_contents_data)}")
        
        if len(overviews) > 0:
            # TF-IDF ë²¡í„°í™”
            print("TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
            tfidf_vectorizer = TfidfVectorizer(
                max_features=3000,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
            
            tfidf_matrix = tfidf_vectorizer.fit_transform(overviews)
            print(f"TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ: {tfidf_matrix.shape}")
        
        return True
        
    except FileNotFoundError:
        print("ì˜¤ë¥˜: web/json/all_contents.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

class MovieKeywordExtractor:
    def __init__(self):
        # ì˜ì–´ ë¶ˆìš©ì–´ ì„¤ì •
        self.stop_words = set(stopwords.words('english'))
        # ì¶”ê°€ ë¶ˆìš©ì–´ (ì˜í™” ê´€ë ¨ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤)
        self.movie_stop_words = {
            'movie', 'film', 'story', 'character', 'plot', 'scene', 
            'drama', 'action', 'comedy', 'thriller', 'horror',
            'one', 'two', 'three', 'first', 'second', 'last',
            'new', 'old', 'good', 'bad', 'big', 'small',
            'man', 'woman', 'people', 'person', 'life', 'time',
            'way', 'world', 'day', 'year', 'home', 'family'
        }
        self.stop_words.update(self.movie_stop_words)
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # í† í°í™”
        tokens = word_tokenize(text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ 3 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
        filtered_tokens = [token for token in tokens 
                          if token not in self.stop_words and len(token) >= 3]
        
        return ' '.join(filtered_tokens)
    
    def extract_keywords_tfidf(self, text, max_features=10):
        """TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        processed_text = self.preprocess_text(text)
        
        if not processed_text.strip():
            return []
        
        try:
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),  # 1-2ê·¸ë¨ ì‚¬ìš©
                min_df=1,
                max_df=0.95
            )
            
            tfidf_matrix = vectorizer.fit_transform([processed_text])
            feature_names = vectorizer.get_feature_names_out()
            scores = tfidf_matrix.toarray()[0]
            
            # ì ìˆ˜ì™€ ë‹¨ì–´ë¥¼ íŠœí”Œë¡œ ë§Œë“¤ì–´ ì •ë ¬
            keyword_scores = list(zip(feature_names, scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ì ìˆ˜ê°€ 0ì¸ ê²ƒë“¤ ì œì™¸
            keywords = [{'word': word, 'score': float(score)} 
                       for word, score in keyword_scores if score > 0]
            
            return keywords[:max_features]
            
        except Exception as e:
            print(f"TF-IDF ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_keywords_frequency(self, text, max_features=10):
        """ë‹¨ìˆœ ë¹ˆë„ìˆ˜ë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        processed_text = self.preprocess_text(text)
        
        if not processed_text.strip():
            return []
        
        try:
            vectorizer = CountVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),
                min_df=1
            )
            
            count_matrix = vectorizer.fit_transform([processed_text])
            feature_names = vectorizer.get_feature_names_out()
            counts = count_matrix.toarray()[0]
            
            keyword_counts = list(zip(feature_names, counts))
            keyword_counts.sort(key=lambda x: x[1], reverse=True)
            
            keywords = [{'word': word, 'count': int(count)} 
                       for word, count in keyword_counts if count > 0]
            
            return keywords[:max_features]
            
        except Exception as e:
            print(f"ë¹ˆë„ìˆ˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

# ì „ì—­ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
extractor = MovieKeywordExtractor()

def calculate_genre_similarity(genres1, genres2):
    """ì¥ë¥´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ì‚¬ìš©)"""
    if not genres1 or not genres2:
        return 0.0
    
    set1 = set(genres1)
    set2 = set(genres2)
    
    # Jaccard ìœ ì‚¬ë„: êµì§‘í•© / í•©ì§‘í•©
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    if union == 0:
        return 0.0
    
    return intersection / union

@app.route('/api/recommend-content', methods=['POST'])
def recommend_content():
    """ì½˜í…ì¸  ì¶”ì²œ API (ì¤„ê±°ë¦¬ + ì¥ë¥´ ê¸°ë°˜)"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    try:
        data = request.get_json()
        
        if not data or 'movie_id' not in data or 'type' not in data:
            return jsonify({'error': 'movie_idì™€ type í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        movie_id = int(data['movie_id'])
        content_type = data['type']
        max_recommendations = data.get('max_recommendations', 10)
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (ì¡°ì • ê°€ëŠ¥)
        overview_weight = data.get('overview_weight', 0.7)  # ì¤„ê±°ë¦¬ ê°€ì¤‘ì¹˜
        genre_weight = data.get('genre_weight', 0.3)        # ì¥ë¥´ ê°€ì¤‘ì¹˜
        
        # ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
        if all_contents_data is None:
            if not load_all_contents():
                return jsonify({'error': 'ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 500
        
        # í˜„ì¬ ì½˜í…ì¸  ì°¾ê¸°
        current_content = None
        current_index = -1
        
        for i, content in enumerate(all_contents_data):
            if content['id'] == movie_id and content['type'] == content_type:
                current_content = content
                current_index = i
                break
        
        if current_content is None:
            return jsonify({'error': 'í•´ë‹¹ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 404
        
        current_overview = current_content.get('overview', '')
        current_genres = current_content.get('genre_ids', [])
        
        if current_overview == '' and not current_genres:
            return jsonify({
                'current_content': {
                    'id': current_content['id'],
                    'title': current_content.get('title', current_content.get('name', 'Unknown')),
                    'type': current_content['type']
                },
                'recommendations': [],
                'message': 'í˜„ì¬ ì½˜í…ì¸ ì— ì¤„ê±°ë¦¬ì™€ ì¥ë¥´ ì •ë³´ê°€ ì—†ì–´ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            })
        
        # TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ê°€ ì—†ë‹¤ë©´ ìƒì„±
        if tfidf_matrix is None:
            return jsonify({'error': 'TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500
        
        # ì¤„ê±°ë¦¬ ìœ ì‚¬ë„ ê³„ì‚°
        overview_similarities = np.zeros(len(all_contents_data))
        if current_overview and current_overview.strip():
            current_vector = tfidf_matrix[current_index:current_index+1]
            overview_similarities = cosine_similarity(current_vector, tfidf_matrix).flatten()
        
        # ëª¨ë“  ì½˜í…ì¸ ì™€ì˜ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°
        similar_contents = []
        for i, content in enumerate(all_contents_data):
            if i == current_index:  # ìê¸° ìì‹ ì€ ì œì™¸
                continue
            
            # ì¥ë¥´ ìœ ì‚¬ë„ ê³„ì‚°
            content_genres = content.get('genre_ids', [])
            genre_similarity = calculate_genre_similarity(current_genres, content_genres)
            
            # ì¤„ê±°ë¦¬ ìœ ì‚¬ë„
            overview_similarity = overview_similarities[i] if current_overview else 0.0
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
            final_similarity = (overview_similarity * overview_weight) + (genre_similarity * genre_weight)
            
            # ìµœì†Œ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ì œì™¸)
            min_threshold = 0.1
            if final_similarity > min_threshold:
                similar_contents.append({
                    'index': i,
                    'similarity': float(final_similarity),
                    'overview_similarity': float(overview_similarity),
                    'genre_similarity': float(genre_similarity),
                    'content': content
                })
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        similar_contents.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        top_recommendations = similar_contents[:max_recommendations]
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        recommendations = []
        for item in top_recommendations:
            content = item['content']
            recommendations.append({
                'id': content['id'],
                'title': content.get('title', content.get('name', 'Unknown')),
                'type': content['type'],
                'poster_path': content.get('poster_path', ''),
                'overview': content.get('overview', ''),
                'vote_average': content.get('vote_average', 0),
                'similarity_score': round(item['similarity'], 4),
                'overview_similarity': round(item['overview_similarity'], 4),
                'genre_similarity': round(item['genre_similarity'], 4),
                'genre_ids': content.get('genre_ids', [])
            })
        
        return jsonify({
            'current_content': {
                'id': current_content['id'],
                'title': current_content.get('title', current_content.get('name', 'Unknown')),
                'type': current_content['type'],
                'genres': current_genres
            },
            'recommendations': recommendations,
            'total_recommendations': len(recommendations),
            'algorithm': 'TF-IDF + Genre Similarity (Hybrid)',
            'weights': {
                'overview_weight': overview_weight,
                'genre_weight': genre_weight
            }
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/extract-keywords', methods=['POST'])
def extract_keywords():
    try:
        data = request.get_json()
        
        if not data or 'overview' not in data:
            return jsonify({'error': 'overview í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        overview = data['overview']
        movie_title = data.get('title', 'Unknown')
        method = data.get('method', 'tfidf')  # 'tfidf' ë˜ëŠ” 'frequency'
        max_keywords = data.get('max_keywords', 10)
        
        if not overview or overview.strip() == '':
            return jsonify({
                'title': movie_title,
                'keywords': [],
                'message': 'ì¤„ê±°ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'
            })
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        if method == 'frequency':
            keywords = extractor.extract_keywords_frequency(overview, max_keywords)
        else:
            keywords = extractor.extract_keywords_tfidf(overview, max_keywords)
        
        return jsonify({
            'title': movie_title,
            'method': method,
            'keywords': keywords,
            'total_keywords': len(keywords)
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/test-weights', methods=['POST'])
def test_weights():
    """ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ë¡œ ì¶”ì²œ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” API"""
    global all_contents_data, tfidf_matrix
    
    try:
        data = request.get_json()
        
        if not data or 'movie_id' not in data or 'type' not in data:
            return jsonify({'error': 'movie_idì™€ type í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        movie_id = int(data['movie_id'])
        content_type = data['type']
        
        # ì—¬ëŸ¬ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
        weight_combinations = [
            {'overview': 1.0, 'genre': 0.0},    # ì¤„ê±°ë¦¬ë§Œ
            {'overview': 0.8, 'genre': 0.2},    # ì¤„ê±°ë¦¬ ì¤‘ì‹¬
            {'overview': 0.7, 'genre': 0.3},    # ê¸°ë³¸ê°’
            {'overview': 0.6, 'genre': 0.4},    # ê· í˜•
            {'overview': 0.5, 'genre': 0.5},    # ì™„ì „ ê· í˜•
            {'overview': 0.3, 'genre': 0.7},    # ì¥ë¥´ ì¤‘ì‹¬
            {'overview': 0.0, 'genre': 1.0}     # ì¥ë¥´ë§Œ
        ]
        
        results = []
        
        for weights in weight_combinations:
            # ê° ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ì¶”ì²œ ìš”ì²­
            test_data = {
                'movie_id': movie_id,
                'type': content_type,
                'max_recommendations': 5,
                'overview_weight': weights['overview'],
                'genre_weight': weights['genre']
            }
            
            # recommend_content í•¨ìˆ˜ ë‚´ë¶€ ë¡œì§ì„ ì¬ì‚¬ìš©
            # (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìƒìœ„ 5ê°œë§Œ ê°€ì ¸ì˜´)
            # ... ì‹¤ì œ êµ¬í˜„ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ìƒëµí•˜ê³  ê²°ê³¼ë§Œ ì €ì¥
            
            results.append({
                'weights': weights,
                'description': f"ì¤„ê±°ë¦¬ {int(weights['overview']*100)}% + ì¥ë¥´ {int(weights['genre']*100)}%",
                'sample_count': 5
            })
        
        return jsonify({
            'movie_id': movie_id,
            'type': content_type,
            'weight_tests': results,
            'recommendation': 'ê°€ì¥ ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì„ íƒí•˜ì—¬ /api/recommend-contentë¥¼ í˜¸ì¶œí•˜ì„¸ìš”'
        })
        
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/content-stats', methods=['GET'])
def content_stats():
    """ì½˜í…ì¸  ë°ì´í„° í†µê³„ ì •ë³´ ì œê³µ"""
    global all_contents_data
    
    if all_contents_data is None:
        return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500
    
    # ê¸°ë³¸ í†µê³„
    total_contents = len(all_contents_data)
    
    # íƒ€ì…ë³„ ë¶„ë¥˜
    type_counts = {}
    genre_counts = {}
    overview_counts = {'with_overview': 0, 'without_overview': 0}
    
    for content in all_contents_data:
        # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        content_type = content.get('type', 'unknown')
        type_counts[content_type] = type_counts.get(content_type, 0) + 1
        
        # ì¥ë¥´ë³„ ì¹´ìš´íŠ¸
        genres = content.get('genre_ids', [])
        for genre_id in genres:
            genre_counts[genre_id] = genre_counts.get(genre_id, 0) + 1
        
        # ì¤„ê±°ë¦¬ ìœ ë¬´
        overview = content.get('overview', '')
        if overview and overview.strip():
            overview_counts['with_overview'] += 1
        else:
            overview_counts['without_overview'] += 1
    
    # ìƒìœ„ 10ê°œ ì¥ë¥´
    top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return jsonify({
        'total_contents': total_contents,
        'type_distribution': type_counts,
        'overview_distribution': overview_counts,
        'top_genres': [{'genre_id': gid, 'count': count} for gid, count in top_genres],
        'tfidf_matrix_shape': list(tfidf_matrix.shape) if tfidf_matrix is not None else None
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    global all_contents_data
    status = {
        'status': 'OK', 
        'message': 'API ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.',
        'data_loaded': all_contents_data is not None,
        'total_contents': len(all_contents_data) if all_contents_data else 0,
        'features': ['ì¤„ê±°ë¦¬ ê¸°ë°˜ ì¶”ì²œ', 'ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ', 'í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ', 'ê°€ì¤‘ì¹˜ ì¡°ì •']
    }
    return jsonify(status)

@app.route('/api/reload-data', methods=['POST'])
def reload_data():
    """ë°ì´í„° ì¬ë¡œë“œ API"""
    global all_contents_data, tfidf_matrix, tfidf_vectorizer
    
    # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
    all_contents_data = None
    tfidf_matrix = None
    tfidf_vectorizer = None
    
    # ë°ì´í„° ì¬ë¡œë“œ
    if load_all_contents():
        return jsonify({'message': 'ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì¬ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.'})
    else:
        return jsonify({'error': 'ë°ì´í„° ì¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 500

# ===== ì‹¤ì œ ê°ì • ì¸ì‹ API =====

def decode_base64_image(image_data):
    """Base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¥¼ ë””ì½”ë”©"""
    try:
        # data:image/jpeg;base64, ë¶€ë¶„ ì œê±°
        if ',' in image_data:
            image_data = image_data.split(',')[1]
        
        # Base64 ë””ì½”ë”©
        img_bytes = base64.b64decode(image_data)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil_image = Image.open(BytesIO(img_bytes))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (RGB -> BGR)
        cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return cv_image
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
        return None

@app.route('/api/analyze-emotion', methods=['POST'])
def analyze_emotion():
    """ì‹¤ì œ ì–¼êµ´ ì´ë¯¸ì§€ì—ì„œ ê°ì • ë¶„ì„"""
    global emotion_detector
    
    try:
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({'error': 'image í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤ (Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€)'}), 400
        
        # ê°ì • ì¸ì‹ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
        if emotion_detector is None:
            logger.info("ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            if not initialize_emotion_detector():
                return jsonify({'error': 'ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'}), 500
        
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        image_data = data['image']
        cv_image = decode_base64_image(image_data)
        
        if cv_image is None:
            return jsonify({'error': 'ì´ë¯¸ì§€ ë””ì½”ë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'}), 400
        
        logger.info(f"ì´ë¯¸ì§€ í¬ê¸°: {cv_image.shape}")
        
        # ì‹¤ì œ ê°ì • ë¶„ì„ ìˆ˜í–‰
        logger.info("ê°ì • ë¶„ì„ ì‹œì‘...")
        emotion_results = emotion_detector.detect_emotions(cv_image)
        
        if not emotion_results:
            return jsonify({
                'success': False,
                'emotions': {},
                'detected_faces': 0
            })
        
        # ê°€ì¥ í° ì–¼êµ´ì˜ ê°ì • ê²°ê³¼ ì‚¬ìš© (ì—¬ëŸ¬ ì–¼êµ´ì´ ê°ì§€ëœ ê²½ìš°)
        main_face = max(emotion_results, key=lambda x: x['box'][2] * x['box'][3])
        emotions = main_face['emotions']
        
        logger.info(f"ê°ì • ë¶„ì„ ì™„ë£Œ: {emotions}")
        
        # ê°ì • ê²°ê³¼ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
        emotion_percentages = {}
        # ì‚¬ìš©í•  ê°ì •ë§Œ í•„í„°ë§ (ë‘ë ¤ì›€, í˜ì˜¤ ì œì™¸)
        allowed_emotions = ['happy', 'sad', 'angry', 'surprised', 'neutral']
        
        for emotion, score in emotions.items():
            if emotion in allowed_emotions:
                emotion_percentages[emotion] = round(score * 100, 1)
        
        # ê°€ì¥ ë†’ì€ ê°ì • ì°¾ê¸° (í—ˆìš©ëœ ê°ì • ì¤‘ì—ì„œ)
        filtered_emotions = {k: v for k, v in emotions.items() if k in allowed_emotions}
        dominant_emotion = max(filtered_emotions.items(), key=lambda x: x[1])
        
        return jsonify({
            'success': True,
            'emotions': emotion_percentages,
            'dominant_emotion': {
                'name': dominant_emotion[0],
                'confidence': round(dominant_emotion[1] * 100, 2)
            },
            'detected_faces': len(emotion_results),
            'face_box': main_face['box'],  # [x, y, width, height]
            'message': f'{dominant_emotion[0]} ê°ì •ì´ {round(dominant_emotion[1] * 100, 1)}% ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤'
        })
        
    except Exception as e:
        logger.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/api/emotion-webcam', methods=['POST'])
def emotion_webcam():
    """ì›¹ìº ì—ì„œ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ (ë‹¨ì¼ í”„ë ˆì„)"""
    global emotion_detector
    
    try:
        data = request.get_json()
        
        if not data or 'frame' not in data:
            return jsonify({'error': 'frame í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        # ê°ì • ì¸ì‹ ëª¨ë¸ í™•ì¸
        if emotion_detector is None:
            if not initialize_emotion_detector():
                return jsonify({'error': 'ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨'}), 500
        
        # í”„ë ˆì„ ë””ì½”ë”©
        frame_data = data['frame']
        cv_frame = decode_base64_image(frame_data)
        
        if cv_frame is None:
            return jsonify({'error': 'í”„ë ˆì„ ë””ì½”ë”© ì‹¤íŒ¨'}), 400
        
        # ë¹ ë¥¸ ê°ì • ë¶„ì„ (ì‹¤ì‹œê°„ ì²˜ë¦¬ìš©)
        try:
            emotion_results = emotion_detector.detect_emotions(cv_frame)
            
            if not emotion_results:
                return jsonify({
                    'success': False,
                    'emotions': {
                        'happy': 0, 'sad': 0, 'angry': 0, 'surprised': 0, 'neutral': 20
                    },
                    'face_detected': False
                })
            
            # ë©”ì¸ ì–¼êµ´ì˜ ê°ì •
            main_face = max(emotion_results, key=lambda x: x['box'][2] * x['box'][3])
            emotions = main_face['emotions']
            
            # í¼ì„¼íŠ¸ ë³€í™˜
            emotion_percentages = {}
            # ì‚¬ìš©í•  ê°ì •ë§Œ í•„í„°ë§ (ë‘ë ¤ì›€, í˜ì˜¤ ì œì™¸)
            allowed_emotions = ['happy', 'sad', 'angry', 'surprised', 'neutral']
            
            for emotion, score in emotions.items():
                if emotion in allowed_emotions:
                    emotion_percentages[emotion] = round(score * 100, 1)
            
            return jsonify({
                'success': True,
                'emotions': emotion_percentages,
                'face_detected': True,
                'face_box': main_face['box']
            })
            
        except Exception as e:
            logger.warning(f"ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ê²½ê³ : {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return jsonify({
                'success': False,
                'emotions': {
                    'happy': 0, 'sad': 0, 'angry': 0, 'surprised': 0, 'neutral': 20
                },
                'face_detected': False,
                'error': str(e)
            })
        
    except Exception as e:
        logger.error(f"ì›¹ìº  ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ì›¹ìº  ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/emotion-test', methods=['GET'])
def emotion_test():
    """ê°ì • ì¸ì‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    global emotion_detector
    
    try:
        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        model_status = emotion_detector is not None
        
        if not model_status:
            init_success = initialize_emotion_detector()
            model_status = init_success
        
        return jsonify({
            'emotion_model_loaded': model_status,
            'available_emotions': [
                'angry', 'happy', 'neutral', 'sad', 'surprised'
            ],
            'model_info': 'FER (Facial Expression Recognition) with MTCNN',
            'endpoints': [
                'POST /api/analyze-emotion - ë‹¨ì¼ ì´ë¯¸ì§€ ê°ì • ë¶„ì„',
                'POST /api/emotion-webcam - ì‹¤ì‹œê°„ ì›¹ìº  í”„ë ˆì„ ë¶„ì„',
                'GET /api/emotion-test - ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸'
            ],
            'status': 'ready' if model_status else 'failed'
        })
        
    except Exception as e:
        logger.error(f"ê°ì • í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return jsonify({
            'emotion_model_loaded': False,
            'error': str(e),
            'status': 'error'
        }), 500

# ===== ê°ì • ê¸°ë°˜ ì˜í™” ì¶”ì²œ API =====

@app.route('/api/recommend-movies', methods=['POST'])
def recommend_movies():
    """ê°ì • ê¸°ë°˜ ì˜í™” ì¶”ì²œ"""
    try:
        data = request.get_json()
        
        if not data or 'emotion' not in data:
            return jsonify({'error': 'emotion í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        emotion = data['emotion']
        confidence = data.get('confidence', 0)
        
        # ê°ì •ë³„ ì¥ë¥´ ë§¤í•‘
        emotion_genre_mapping = {
            'happy': [35, 10749, 12, 16],  # ì½”ë¯¸ë””, ë¡œë§¨ìŠ¤, ì–´ë“œë²¤ì²˜, ì• ë‹ˆë©”ì´ì…˜
            'sad': [18, 10749, 10402],      # ë“œë¼ë§ˆ, ë¡œë§¨ìŠ¤, ìŒì•…
            'angry': [28, 53, 80],          # ì•¡ì…˜, ìŠ¤ë¦´ëŸ¬, ë²”ì£„
            'surprised': [28, 53, 27, 878], # ì•¡ì…˜, ìŠ¤ë¦´ëŸ¬, í˜¸ëŸ¬, SF
            'neutral': [28, 35, 18, 12]     # ì•¡ì…˜, ì½”ë¯¸ë””, ë“œë¼ë§ˆ, ì–´ë“œë²¤ì²˜
        }
        
        # ê°ì •ë³„ ë©”ì‹œì§€
        emotion_messages = {
            'happy': f'ğŸ˜Š ê¸°ìœ ê°ì •ì´ {confidence}% ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! ì¦ê±°ìš´ ì˜í™”ë“¤ì„ ì¶”ì²œë“œë ¤ìš”.',
            'sad': f'ğŸ˜¢ ìŠ¬í”ˆ ê°ì •ì´ {confidence}% ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ë™ì ì¸ ì˜í™”ë¡œ ë§ˆìŒì„ ë‹¬ë˜ë³´ì„¸ìš”.',
            'angry': f'ğŸ˜  í™”ë‚œ ê°ì •ì´ {confidence}% ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì•¡ì…˜ ì˜í™”ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í’€ì–´ë³´ì„¸ìš”!',
            'surprised': f'ğŸ˜® ë†€ë€ ê°ì •ì´ {confidence}% ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤ë¦´ ë„˜ì¹˜ëŠ” ì˜í™”ë¥¼ ì¶”ì²œë“œë ¤ìš”!',
            'neutral': f'ğŸ˜ ì°¨ë¶„í•œ ìƒíƒœì…ë‹ˆë‹¤. ì¸ê¸° ì˜í™”ë“¤ì„ ê³¨ë¼ë³´ì„¸ìš”.'
        }
        
        # í•´ë‹¹ ê°ì •ì— ë§ëŠ” ì¥ë¥´ IDë“¤
        target_genres = emotion_genre_mapping.get(emotion, [28, 35, 18])
        
        # ì½˜í…ì¸  ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
        if all_contents_data is None:
            if not load_all_contents():
                return jsonify({'error': 'ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}), 500
        
        # í•´ë‹¹ ì¥ë¥´ì˜ ì˜í™”ë“¤ í•„í„°ë§ (ì˜í™”ë§Œ, ë†’ì€ í‰ì )
        recommended_movies = []
        for content in all_contents_data:
            if (content.get('type') == 'movie' and 
                content.get('vote_average', 0) >= 6.0 and
                content.get('genre_ids')):
                
                # ì¥ë¥´ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                content_genres = content.get('genre_ids', [])
                if any(genre in target_genres for genre in content_genres):
                    recommended_movies.append(content)
        
        # í‰ì  ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 12ê°œ ì„ íƒ
        recommended_movies.sort(key=lambda x: x.get('vote_average', 0), reverse=True)
        recommended_movies = recommended_movies[:12]
        
        # ì˜í™” ì¹´ë“œ HTML ìƒì„±
        movie_cards = []
        for movie in recommended_movies:
            poster_path = movie.get('poster_path', '')
            if poster_path:
                poster_url = f"https://image.tmdb.org/t/p/w500{poster_path}"
            else:
                poster_url = "https://via.placeholder.com/500x750?text=No+Image"
            
            card_html = f"""
            <div class="movie-card" onclick="window.open('movie_detail/detail.html?id={movie["id"]}&type=movie', '_blank')">
                <img src="{poster_url}" alt="{movie.get('title', 'Unknown')}" class="movie-poster">
                <div class="movie-title">{movie.get('title', 'Unknown')}</div>
                <div class="movie-rating">â­ {movie.get('vote_average', 0):.1f}</div>
                <div class="movie-genre">ê°œë´‰ì¼: {movie.get('release_date', 'Unknown')}</div>
            </div>
            """
            movie_cards.append(card_html)
        
        return jsonify({
            'success': True,
            'emotion': emotion,
            'confidence': confidence,
            'message': emotion_messages.get(emotion, 'ì˜í™”ë¥¼ ì¶”ì²œë“œë ¤ìš”!'),
            'total_movies': len(recommended_movies),
            'movie_cards': movie_cards
        })
        
    except Exception as e:
        logger.error(f"ì˜í™” ì¶”ì²œ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ì˜í™” ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

if __name__ == '__main__':
    print("ğŸ­ ì˜í™” ì¶”ì²œ + ì‹¤ì œ ê°ì • ì¸ì‹ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("\nğŸ“½ï¸ ì˜í™” ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸:")
    print("- POST /api/extract-keywords")
    print("- POST /api/recommend-content (ì¤„ê±°ë¦¬ + ì¥ë¥´ í•˜ì´ë¸Œë¦¬ë“œ)")
    print("- POST /api/test-weights (ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸)")
    print("- GET /api/content-stats (ë°ì´í„° í†µê³„)")
    print("- POST /api/reload-data")
    print("- GET /api/health")
    
    print("\nğŸ¤– ì‹¤ì œ ê°ì • ì¸ì‹ ì—”ë“œí¬ì¸íŠ¸:")
    print("- POST /api/analyze-emotion (ì´ë¯¸ì§€ ê°ì • ë¶„ì„)")
    print("- POST /api/emotion-webcam (ì‹¤ì‹œê°„ ì›¹ìº  ê°ì • ë¶„ì„)")
    print("- GET /api/emotion-test (ê°ì • ì¸ì‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸)")
    print("- POST /api/recommend-movies (ê°ì • ê¸°ë°˜ ì˜í™” ì¶”ì²œ)")
    
    print("\nğŸ”¥ ì£¼ìš” ê¸°ëŠ¥:")
    print("âœ… ì¥ë¥´ ìœ ì‚¬ë„ ì¶”ê°€ (Jaccard ìœ ì‚¬ë„)")
    print("âœ… ì¤„ê±°ë¦¬ + ì¥ë¥´ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ")
    print("âœ… ì‹¤ì œ AI ê°ì • ì¸ì‹ (FER + MTCNN)")
    print("âœ… ì‹¤ì‹œê°„ ì›¹ìº  ê°ì • ë¶„ì„")
    print("âœ… ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’: ì¤„ê±°ë¦¬ 70% + ì¥ë¥´ 30%)")
    
    # ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“¦ ì½˜í…ì¸  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    load_all_contents()
    
    # ê°ì • ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”
    print("\nğŸ¤– ê°ì • ì¸ì‹ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
    initialize_emotion_detector()
    
    print("\nğŸš€ ì„œë²„ ì‹œì‘!")
    app.run(debug=True, host='0.0.0.0', port=5000) 